# ğŸš€ SEC EDGAR Historical Data Collection System

## ğŸ“Š Project Overview

This system collects **ALL SEC filings from the 21st century (2000-present)** for **6,700+ publicly traded companies**. This represents a massive dataset of financial documents that will provide unprecedented training data for financial language models.

## ğŸ¯ Scope & Scale

### **Data Collection Targets**
- **ğŸ“… Time Period**: January 1, 2000 â†’ Present (25+ years)
- **ğŸ¢ Companies**: 6,700+ publicly traded companies
- **ğŸ“„ Filing Types**: 10-K, 10-Q, 8-K (annual, quarterly, current reports)
- **ğŸ“Š Estimated Filings**: 500,000 - 2,000,000+ individual filings
- **ğŸ’¾ Storage**: 10-50+ GB of structured data

### **API Requirements**
- **ğŸŒ SEC EDGAR API**: 10 requests/second rate limit
- **ğŸ”‘ Authentication**: Requires User-Agent header
- **âš¡ Processing**: 670,000+ API calls required
- **â±ï¸ Timeline**: 48-72 hours for full collection

## ğŸš€ Quick Start Guide

### **1. Install Dependencies**
```bash
cd /Users/alex/relentless/model/data
pip3 install -r requirements.txt
```

### **2. Test the System**
```bash
# Quick test with 5 major companies (2020-present)
python3 edgar_fetcher.py sample

# Demo with 3 companies (recent filings)
python3 edgar_fetcher.py demo
```

### **3. Full Historical Collection**
```bash
# Interactive mode - requires confirmation
python3 edgar_fetcher.py

# Or use the convenience script
./fetch_sec_data.sh main
```

## ğŸ“‹ Available Commands

| Command | Description | Scale | Time | Use Case |
|---------|-------------|-------|------|----------|
| `sample` | 5 major companies, 2020-present | Small | 2-3 min | Testing |
| `demo` | 3 companies, recent filings | Small | 1-2 min | Demo |
| `main` | ALL 6700+ companies, 2000-present | Massive | 48-72h | Full training data |
| `resume` | Resume interrupted collection | Variable | Variable | Recovery |

## ğŸ“ Data Organization

### **Directory Structure**
```
SEC/
â”œâ”€â”€ CIK0000320193_submissions.json  # Company submission metadata
â”œâ”€â”€ CIK0000789019_submissions.json
â”œâ”€â”€ ...
â”œâ”€â”€ historical_results_batch_001.json  # Incremental results
â”œâ”€â”€ historical_results_batch_002.json
â”œâ”€â”€ ...
â”œâ”€â”€ all_historical_data.json           # Complete dataset
â”œâ”€â”€ collection_summary.json            # Statistics & reports
â”œâ”€â”€ cik_mapping.json                   # CIK cache
â”œâ”€â”€ fetch_progress.json               # Progress tracking
â”œâ”€â”€ known_ciks.json                   # Built-in CIK database
â””â”€â”€ historical_collection_summary.json # Comprehensive report
```

### **Data Format**
```json
{
  "AAPL": {
    "cik": "0000320193",
    "ticker": "AAPL",
    "filings": [
      {
        "form": "10-K",
        "filingDate": "2024-09-28",
        "accessionNumber": "0000320193-24-000123",
        "primaryDocument": "aapl-20240928.htm",
        "reportDate": "2024-09-28",
        "size": 1234567
      },
      {
        "form": "10-Q",
        "filingDate": "2024-08-03",
        "accessionNumber": "0000320193-24-000089",
        "primaryDocument": "aapl-20240803.htm",
        "reportDate": "2024-06-29",
        "size": 890123
      }
    ],
    "total_filings_found": 96,
    "fetch_time": "2025-09-21T06:07:41",
    "success": true
  }
}
```

## ğŸ¯ Collection Features

### **âœ… Robust Error Handling**
- **ğŸ”„ Automatic Retries**: Exponential backoff for failed requests
- **ğŸ“Š Progress Tracking**: Save progress every 10 companies
- **ğŸ” Resume Capability**: Continue from interruptions
- **âš ï¸ Graceful Degradation**: Handle missing documents

### **âœ… Rate Limiting Compliance**
- **â±ï¸ 10 requests/second**: Strictly respects SEC limits
- **ğŸ• Delays**: 0.5s between companies, 2s between batches
- **ğŸ“ˆ Monitoring**: Real-time progress and ETA calculation

### **âœ… Comprehensive Reporting**
- **ğŸ“ˆ Success Metrics**: Detailed statistics and success rates
- **ğŸ“Š Form Breakdown**: Count by filing type (10-K, 10-Q, 8-K)
- **ğŸ¢ Top Companies**: Companies with most filings
- **ğŸ“… Timeline Distribution**: Filing counts by year

### **âœ… Data Quality**
- **ğŸ” Date Filtering**: Precise date range filtering
- **ğŸ“‹ Form Validation**: Only collect specified filing types
- **ğŸ“ Metadata Extraction**: Comprehensive filing information
- **ğŸ—‚ï¸ Organization**: Structured by company and date

## ğŸš¨ Important Considerations

### **âš ï¸ System Requirements**
- **ğŸ’» Hardware**: Modern computer with good internet connection
- **ğŸ’¾ Storage**: 10-50+ GB available disk space
- **ğŸŒ Network**: Stable internet (will make 670k+ API calls)
- **â±ï¸ Time**: 48-72 hours for complete collection
- **ğŸ”‹ Power**: Ensure system won't sleep/hibernate

### **ğŸ“‹ Before Starting**
1. **Backup**: Ensure you have adequate disk space
2. **Network**: Test internet connectivity
3. **Power**: Disable sleep mode
4. **Monitoring**: Consider running in screen/tmux for remote monitoring

### **ğŸ›ï¸ Control & Monitoring**
- **ğŸ“Š Real-time Progress**: Shows current batch, ETA, success rate
- **ğŸ”„ Resume Support**: Can restart from interruptions
- **âš¡ Speed Control**: Configurable batch sizes and delays
- **ğŸ“ˆ Statistics**: Comprehensive reporting at completion

## ğŸ“Š Expected Results

### **Sample Output Statistics**
```json
{
  "collection_summary": {
    "start_date": "2000-01-01",
    "end_date": "2025-09-21",
    "total_tickers_processed": 6700,
    "successful_companies": 6500,
    "failed_companies": 200,
    "success_rate": "97.0%",
    "total_filings_collected": 850000,
    "average_filings_per_company": 127
  },
  "form_type_breakdown": {
    "10-K": 162000,
    "10-Q": 486000,
    "8-K": 202000
  }
}
```

### **Timeline Distribution**
- **ğŸ“ˆ 2000-2010**: Building phase (~50k filings)
- **ğŸ“Š 2010-2020**: Growth phase (~300k filings)
- **ğŸš€ 2020-Present**: Modern era (~500k filings)

## ğŸ¯ Use Cases

### **ğŸ¤– Machine Learning Training**
- **Language Models**: Financial document understanding
- **Classification**: Filing type and content classification
- **Information Extraction**: Financial data extraction
- **Sentiment Analysis**: Market sentiment from filings

### **ğŸ“Š Financial Analysis**
- **Company Research**: Historical filing analysis
- **Trend Analysis**: Multi-year financial trends
- **Comparative Analysis**: Cross-company comparisons
- **Risk Assessment**: Financial risk indicators

### **ğŸ” Research Applications**
- **Academic Research**: Financial text analysis
- **Regulatory Analysis**: SEC filing patterns
- **Market Research**: Industry trend analysis
- **Compliance**: Regulatory filing studies

## ğŸš¨ Safety & Best Practices

### **âœ… SEC Compliance**
- **â±ï¸ Rate Limiting**: Strictly follows 10 req/sec limit
- **ğŸ‘¤ Proper Headers**: Includes required User-Agent
- **ğŸ”„ Error Handling**: Graceful handling of failures
- **ğŸ“Š Monitoring**: Real-time progress tracking

### **ğŸ›¡ï¸ System Protection**
- **ğŸ’¾ Incremental Saves**: Progress saved every 10 companies
- **ğŸ”„ Resume Support**: Can restart from interruptions
- **âš¡ Configurable Speed**: Adjustable batch sizes and delays
- **ğŸ“Š Resource Monitoring**: Memory and disk usage tracking

### **ğŸ¯ Data Quality**
- **ğŸ” Validation**: CIK verification and data validation
- **ğŸ“… Date Filtering**: Precise date range enforcement
- **ğŸ“‹ Form Filtering**: Only specified filing types
- **ğŸ“ Metadata**: Comprehensive filing information

## ğŸš€ Ready to Start?

1. **Test First**: Run `python3 edgar_fetcher.py sample` to verify setup
2. **Check Resources**: Ensure adequate disk space and stable internet
3. **Start Collection**: Run `python3 edgar_fetcher.py` for full collection
4. **Monitor Progress**: Check logs and progress files regularly
5. **Use Results**: Leverage this massive dataset for ML training!

**This system will provide the most comprehensive SEC filing dataset available for financial AI training!** ğŸ“ˆğŸ¤–

## ğŸ“ Support

For issues or questions:
- Check the logs for detailed error messages
- Review `historical_collection_summary.json` for statistics
- Use `resume` command to continue interrupted collections
- Monitor system resources during long-running operations
